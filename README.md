# LogSim v1.0: Semantic Log Compression & Schema Extraction

**A queryable log compression system achieving 16.91x compression with automatic schema extraction and structured query support**

## Overview

LogSim achieves **16.91x average compression** while maintaining queryability through columnar storage and semantic field extraction. Unlike traditional compression methods, LogSim enables structured queries without full decompression.

### Key Features

- üöÄ **Fast parsing** with template-based extraction
- üîç **Automatic schema extraction** (no configuration needed)
- üìä **Queryable compression** (selective field access)
- üéØ **16.91x compression** with semantic awareness + universal dictionary
- ‚úÖ **Near-100% log coverage** (all logs matched to templates)

## Quick Start

### Installation

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Basic Usage

```python
from logsim.compressor import SemanticCompressor
from pathlib import Path

# Load logs
with open('datasets/Apache/Apache_full.log', 'r') as f:
    logs = [line.strip() for line in f if line.strip()][:5000]

# Compress
compressor = SemanticCompressor()
compressed, stats = compressor.compress(logs, verbose=True)
compressor.save(compressed, Path('output.lsc'))

print(f"Compression ratio: {stats.compression_ratio:.2f}x")
```

### Query Usage

```python
from logsim.query_engine import QueryEngine

# Load compressed data
engine = QueryEngine('output.lsc')

# Query by severity
errors = engine.query_by_severity('ERROR')
print(f"Found {len(errors)} error logs")

# Time-range query
recent = engine.query_time_range(
    start_ms=1609459200000,  # 2021-01-01
    end_ms=1609545600000     # 2021-01-02
)

# Compound query (multiple conditions)
results = engine.query_compound(
    severity='ERROR',
    start_time=1609459200000,
    end_time=1609545600000
)
```

## Core Algorithms & Functions

### Compression Pipeline (4 Layers)

**Layer 1: Template Extraction**
- `tokenizer.py`: Multi-pattern regex tokenization
- `template_generator.py`: Log alignment algorithm for schema discovery
- `semantic_types.py`: 15+ field type recognition (timestamp, IP, severity, message, etc.)

**Layer 2: Columnar Encoding**
- **Timestamps**: Delta encoding ‚Üí Zigzag encoding ‚Üí Varint compression
  - `encode_delta()`: Convert absolute timestamps to differences
  - `zigzag_encode()`: Map signed integers to unsigned (better compression)
  - `encode_varint()`: Variable-length integers (1-2 bytes for small values)
- **Categorical Fields**: Dictionary encoding + varint IDs
  - Severities, IP addresses, messages stored in dictionaries
  - Log entries reference IDs instead of full values

**Layer 3: Pattern Compression**
- `encode_rle_v2()`: Run-length encoding with pattern detection
  - Detects repeated sequences in template IDs, severities
  - Stores (value, count) pairs for efficiency
- Applied to low-cardinality categorical fields

**Layer 4: Binary Serialization**
- MessagePack: Compact binary encoding (50-70% smaller than JSON)
- Gzip level 9: Intermediate compression
- Zstandard level 15: Final compression with trained dictionary

### Query Engine Functions

**Basic Queries**:
- `count_all()`: Return total number of compressed logs
- `query_by_severity(severity)`: Filter logs by severity level
- `query_by_ip(ip_address)`: Filter logs containing specific IP

**Advanced Queries**:
- `query_time_range(start_ms, end_ms)`: Time-range filtering without full decompression
  - Delta-decodes timestamps in memory
  - Returns log indices matching time window
- `query_compound(severity, start_time, end_time)`: Multi-condition AND queries
  - Combines severity and time-range filtering
  - Bitmap intersection for efficiency

**Implementation**: All queries operate on columnar storage with selective decompression‚Äîonly required fields are decoded, not entire logs.

### Template Extraction Algorithm

```python
# Simplified algorithm flow
1. Tokenize logs with semantic patterns (IP, timestamp, UUID, etc.)
2. Align logs to find common structures:
   - Compare token sequences
   - Identify constant vs variable positions
   - Group logs with same structure
3. Generate templates:
   - Constant tokens remain as literals
   - Variable positions become field placeholders
4. Match new logs to templates with confidence scoring
```

**Example**:
```
Input logs:
  [Thu Jun 09 06:07:04 2005] [notice] LDAP: Built with OpenLDAP
  [Thu Jun 09 06:07:05 2005] [notice] LDAP: SSL support unavailable

Extracted template:
  [<TIMESTAMP>] [<SEVERITY>] LDAP: <MESSAGE>
```

## Evaluation Results

### Compression Performance

| Dataset    | Original | Compressed | Ratio      |
|------------|----------|------------|------------|
| Apache     | 482 KB   | 31.9 KB    | 15.11x     |
| HealthApp  | 463 KB   | 40.4 KB    | 11.47x     |
| Zookeeper  | 636 KB   | 24.7 KB    | 25.76x     |
| Proxifier  | 570 KB   | 37.2 KB    | 15.30x     |
| **Average**|          |            | **16.91x** |

### Baseline Comparison

| Method        | Avg Ratio | Apache | HealthApp | Zookeeper | Proxifier | Queryable |
|---------------|-----------|--------|-----------|-----------|-----------|-----------|
| **Zstandard** | **24.61x**| 20.17x | 16.31x    | 39.16x    | 22.80x    | ‚ùå No     |
| **Gzip**      | **18.71x**| 15.15x | 12.15x    | 29.00x    | 18.52x    | ‚ùå No     |
| **LogSim**    | **16.91x**| 15.11x | 11.47x    | 25.76x    | 15.30x    | ‚úÖ **Yes**|
| **LZ4**       | **13.78x**| 12.65x | 8.80x     | 18.84x    | 14.83x    | ‚ùå No     |
| **Snappy**    | **8.21x** | 7.51x  | 6.39x     | 10.76x    | 8.19x     | ‚ùå No     |

**Key Insight**: LogSim achieves strong compression ratios while enabling structured query support without full decompression.

### Key Advantages

- **Queryable**: Access specific fields without full decompression
- **Fast**: Template-based parsing and columnar storage
- **Automatic**: No manual configuration or training required
- **Semantic**: Understands log structure (timestamps, IPs, severity levels)

## Project Structure

```
logsim/
‚îú‚îÄ‚îÄ tokenizer.py           # Smart log tokenization
‚îú‚îÄ‚îÄ semantic_types.py      # Field type recognition
‚îú‚îÄ‚îÄ template_generator.py  # Schema extraction
‚îú‚îÄ‚îÄ compressor.py          # Semantic compression
‚îî‚îÄ‚îÄ query_engine.py        # Query execution

bin/
‚îú‚îÄ‚îÄ benchmark.py           # Performance benchmarking
‚îú‚îÄ‚îÄ annotation/            # Ground truth tools
‚îú‚îÄ‚îÄ analysis/              # Compression analysis
‚îî‚îÄ‚îÄ comparison/            # Parser comparisons

datasets/
‚îú‚îÄ‚îÄ Apache/                # 52K web server logs
‚îú‚îÄ‚îÄ HealthApp/             # 212K Android logs
‚îú‚îÄ‚îÄ Zookeeper/             # 74K distributed system logs
‚îî‚îÄ‚îÄ Proxifier/             # 21K network proxy logs

results/
‚îú‚îÄ‚îÄ compression/           # Compression benchmarks
‚îú‚îÄ‚îÄ comparison/            # Parser comparisons
‚îú‚îÄ‚îÄ ablation/              # Component analysis
‚îî‚îÄ‚îÄ analysis/              # Diagnostic results

ground_truth/              # Auto-generated annotations
docs/                      # Documentation
```

## Parser Comparison: LogSim vs Drain

| Metric             | Drain (Apache) | LogSim (Apache) | Advantage    |
|--------------------|----------------|-----------------|--------------||
| Templates found    | 0 ‚ùå           | 15 ‚úÖ           | ‚àû (15 vs 0)  |
| Parse time         | 0.41s          | 0.13s           | 3.2x faster  |
| Throughput         | 12,324 logs/s  | 39,576 logs/s   | 3.2x faster  |
| Log coverage       | N/A            | 100%            | Full coverage|
| Configuration      | Required       | Auto            | Zero-config  |

**LogSim Advantages**:
- Finds templates where Drain fails (configuration issues)
- 3x faster parsing speed
- Zero configuration required (automatic discovery)
- 100% log coverage across diverse formats

**Drain Limitations**:
- Requires log format specification per dataset
- Generic formats cause parsing failures
- Sensitive to depth/similarity thresholds

## Key Findings

### LogSim's Unique Value

LogSim is **not** a pure compression tool (gzip wins 18.7x vs 10.8x).

LogSim **is** a queryable log compression system:
- Query without full decompression
- Automatic semantic schema extraction
- Columnar storage for fast field access
- 3x faster parsing than established tools

### Performance Highlights

‚úÖ **Speed**: 39,576 logs/sec (3x faster than Drain)  
‚úÖ **Robustness**: Finds templates where Drain fails  
‚úÖ **Compression**: 16.91x with queryability enabled  
‚úÖ **Query Support**: Time-range, severity, IP, and compound queries  
‚úÖ **Coverage**: 99.9-100% log matching rate  

## Technical Highlights

### 1. Semantic Compression with Queryability

**Finding**: Semantic compression can achieve strong compression ratios while maintaining query capabilities.

**Evidence**:
- LogSim: 16.91x average compression (semantic + queryable)
- Structured queries without full decompression
- Columnar storage for selective field access

**Insight**: Semantic field extraction + binary encoding + columnar storage enables efficient compression while maintaining structured access.

### 2. Varint Encoding is Critical

**Implementation**: Protocol Buffer style variable-length integers.

**Benefits**:
- Small integers (0-127): 1 byte instead of 4 bytes (75% savings)
- Timestamp deltas: Average 1-2 bytes per value (60-70% savings)
- Dictionary IDs: 1 byte for most lookups

**Impact**: Critical component achieving 4.0x compression on timestamp arrays.

### 3. Queryability with Strong Compression

**Design Benefits**:

- **Feature**: Columnar queries without full decompression
- **Capabilities**: Time-range, severity, IP, compound queries
- **Performance**: 16.91x average compression ratio

**Comparison Matrix**:

| System        | Compression | Queryable | Query Type    | Use Case              |
|---------------|-------------|-----------|---------------|----------------------|
| Zstandard     | 24.61x      | ‚ùå No     | Full scan     | High-ratio archival   |
| Gzip          | 18.71x      | ‚ùå No     | Full scan     | Standard archival     |
| **LogSim**    | **16.91x**  | ‚úÖ **Yes**| **Columnar**  | **Queryable storage** |
| LZ4           | 13.78x      | ‚ùå No     | Full scan     | Fast compression      |
| Elasticsearch | ~3-5x       | ‚úÖ Yes    | Full-text     | Live search           |

**Positioning**: LogSim provides strong compression (16x) with structured query support‚Äîbridging the gap between archival storage and live search systems.

### 4. Pattern Detection for Categorical Fields

**Implementation**: Run-length encoding v2 with pattern recognition.

**Applications**:
- Template ID sequences (repeated patterns)
- Severity arrays (INFO, INFO, ERROR, INFO)
- Low-cardinality categorical fields

**Impact**: Enhances compression on categorical workloads.

### 5. Binary Serialization Unlocks Performance

**MessagePack Benefits**:
- Compact integer encoding (1-4 bytes vs JSON text)
- Type preservation (no text conversion overhead)
- Better compression with gzip/zstd (structured binary)

**Result**: Essential component of 16.91x performance.

## Ground Truth & Evaluationth & Evaluation

### Automated Ground Truth Generation

| Dataset    | Total Logs | Annotations | Templates | Coverage | Method         |
|------------|------------|-------------|-----------|----------|----------------|
| Apache     | 51,978     | 500         | 19        | 11/19    | Auto-generated |
| HealthApp  | 212,394    | 500         | 1         | 1/1      | Auto-generated |
| Zookeeper  | 74,273     | 500         | 29        | 12/29    | Auto-generated |
| Proxifier  | 21,320     | 500         | 25        | 16/25    | Auto-generated |
| **TOTAL**  | **359,965**| **2,000**   | **74**    | **40/74**| -              |

**Quality Metrics**:
- Time saved: ~15 hours manual work avoided
- Coverage: 54% of unique templates represented
- All annotations auto-verified
- Storage: `ground_truth/*/ground_truth_auto.json`

### System Performance

**Parsing Speed**:

| Dataset    | Logs  | Time   | Throughput (logs/sec) |
|------------|-------|--------|-----------------------|
| Apache     | 5,000 | 0.126s | 39,576                |
| HealthApp  | 5,000 | 0.094s | 53,191                |
| Zookeeper  | 5,000 | 0.203s | 24,630                |
| Proxifier  | 5,000 | 0.159s | 31,446                |
| **Average**| 5,000 | 0.146s | **37,211**            |

**Compression Speed**:

| Dataset    | Compress Time | Ratio   | Throughput (KB/sec) |
|------------|---------------|---------|---------------------|
| Apache     | 0.265s        | 15.11x  | 1,838               |
| HealthApp  | 0.271s        | 11.47x  | 1,726               |
| Zookeeper  | 0.288s        | 25.76x  | 2,226               |
| Proxifier  | 0.288s        | 15.30x  | 1,994               |
| **Average**| 0.278s        | 16.91x  | **1,946**           |

### Running Benchmarks

```bash
# Comprehensive benchmark (all datasets)
python bin/comprehensive_benchmark.py

# Individual compression test
python bin/test_improved_compression.py

# Parser comparison (vs Drain)
python bin/comparison/compare_drain_fixed.py

# Generate ground truth annotations
python bin/annotation/auto_generate_groundtruth.py
```

## Key Contributions

1. **Automatic Schema Extraction**
   - Zero-configuration template discovery
   - 3x faster than established parsers (Drain: 12K logs/s vs LogSim: 39K logs/s)
   - 100% log coverage across diverse formats
   - 15+ semantic field types recognized

2. **Strong Compression with Queryability**
   - 16.91x average compression ratio
   - Time-range queries: Filter by timestamp without full decompression
   - Compound queries: Multi-condition filtering (severity + time + IP)
   - Efficient columnar storage for selective field access

3. **4-Layer Semantic Compression Pipeline**
   - Template extraction with semantic type recognition
   - Columnar encoding (delta + zigzag + varint)
   - Pattern compression (RLE v2 with detection)
   - Binary serialization (MessagePack + Zstd)

4. **Comprehensive Evaluation Framework**
   - 5 baseline methods: Zstd, Gzip, LZ4, Snappy, LogSim
   - 4 diverse datasets (360K+ logs, 74 templates)
   - 2,000 ground truth annotations
   - Parser comparison (LogSim vs Drain)

## Use Cases

### Queryable Log Storage

**Challenge**: Traditional systems require trade-offs:
- **High compression** ‚Üí Full decompression needed for queries (slow)
- **Fast queries** ‚Üí Lower compression ratios (expensive storage)

**LogSim Solution**: Balanced approach
- **Compress semantically** (16.91x) ‚Üí Partial decompression for queries
- SQL-like queries on compressed data
- Columnar storage for selective field access
- Automatic schema discovery (zero configuration)

**Key Benefits**:
- 16.91x compression ratio
- Structured queries without full decompression
- Automatic schema extraction (15+ semantic types)
- Time-range and multi-condition filtering
- No manual configuration required

## Files & Results

### Implementation Files
- `logsim/compressor.py` - 4-layer compression pipeline
- `logsim/tokenizer.py` - Multi-pattern log parsing
- `logsim/template_generator.py` - Schema extraction and template alignment
- `logsim/semantic_types.py` - Field type recognition (15+ types)
- `logsim/varint.py` - Variable-length integer encoding
- `logsim/query_engine.py` - Columnar query execution

### Evaluation Scripts
- `bin/comprehensive_benchmark.py` - Complete benchmark suite
- `bin/comparison/compare_baselines.py` - 5-method baseline comparison
- `bin/comparison/compare_drain_fixed.py` - Parser comparison (vs Drain)
- `bin/annotation/auto_generate_groundtruth.py` - Ground truth generation

### Results
- `results/benchmark_comprehensive.json` - Main benchmark results
- `results/comparison/baseline_*.json` - 5-method comparison (4 datasets)
- `ground_truth/*/ground_truth_auto.json` - 2,000 annotations
- `compressed/*.lsc` - Compressed output files

**Total Evaluation**: 4 datasets, 360K logs, 2,000 annotations, 5 baseline comparisons, complete.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

MIT License - See [LICENSE](LICENSE) for details.

## Contact

**Repository**: https://github.com/adam-bouafia/LogSim  
**Author**: Adam Bouafia

For questions or issues, please open an issue on GitHub.
