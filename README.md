# LogSim - Semantic Log Compression System

[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](logsim/tests/)
[![Coverage](https://img.shields.io/badge/coverage-42%25-yellow.svg)](htmlcov/index.html)
[![Python](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

**Master's Thesis Research Project**: Automatic schema extraction from unstructured system logs using constraint-based parsing and semantic-aware compression.

## ğŸ¯ Research Goals

- **Automatic Schema Discovery**: Extract implicit log schemas without manual annotation
- **Semantic-Aware Compression**: Achieve 8-30Ã— compression while maintaining queryability
- **Real-World Validation**: Tested on diverse log sources (2M+ entries)

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/adam-bouafia/LogSim.git
cd LogSim

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -e .
```

### Interactive Mode (Recommended)

```bash
# Beautiful terminal UI with dataset auto-discovery
python -m logsim.cli.interactive
```

**Features**:
- ğŸ” Auto-discovers datasets in `data/datasets/`
- ğŸ“Š Real-time compression progress
- ğŸ¨ Rich terminal UI with tables and progress bars
- âš¡ Query compressed logs interactively

### Command-Line Usage

```bash
# Compress logs
python -m logsim compress \
  -i data/datasets/Apache/Apache_full.log \
  -o evaluation/compressed/apache.lsc \
  --min-support 3 \
  -m

# Query compressed logs
python -m logsim query \
  -c evaluation/compressed/apache.lsc \
  --severity ERROR \
  --limit 20

# Run full evaluation
python evaluation/run_full_evaluation.py
```

### Docker Usage

```bash
# Interactive mode (Python rich UI)
docker-compose -f deployment/docker-compose.yml run --rm logsim-interactive

# Bash menu (alternative)
docker-compose -f deployment/docker-compose.yml run --rm logsim-interactive-bash

# Run specific command
docker-compose -f deployment/docker-compose.yml run --rm logsim-cli \
  compress -i /app/data/datasets/Apache/Apache_full.log -o /app/evaluation/compressed/apache.lsc -m
```

## ğŸ“ Project Structure (MCP Architecture)

```
LogSim/
â”œâ”€â”€ logsim/                  # Core Python package (Model-Context-Protocol)
â”‚   â”œâ”€â”€ models/             # Data structures (Token, LogTemplate, CompressedLog)
â”‚   â”œâ”€â”€ protocols/          # Abstract interfaces (EncoderProtocol, CompressorProtocol)
â”‚   â”œâ”€â”€ context/           # Business logic
â”‚   â”‚   â”œâ”€â”€ tokenization/  # Smart log tokenization (FSM-based)
â”‚   â”‚   â”œâ”€â”€ extraction/    # Template generation (log alignment algorithm)
â”‚   â”‚   â”œâ”€â”€ classification/# Semantic type recognition (pattern-based)
â”‚   â”‚   â””â”€â”€ encoding/      # Compression codecs (delta, dictionary, varint)
â”‚   â”œâ”€â”€ services/          # High-level orchestration
â”‚   â”‚   â”œâ”€â”€ compressor.py  # 6-stage compression pipeline
â”‚   â”‚   â”œâ”€â”€ query_engine.py# Queryable decompression
â”‚   â”‚   â””â”€â”€ evaluator.py   # Accuracy metrics vs ground truth
â”‚   â”œâ”€â”€ cli/              # User interfaces
â”‚   â”‚   â”œâ”€â”€ interactive.py # Rich terminal UI
â”‚   â”‚   â””â”€â”€ commands.py    # Click-based CLI
â”‚   â””â”€â”€ tests/            # Test suite (25 tests, 100% passing)
â”‚       â”œâ”€â”€ unit/         # Component testing
â”‚       â”œâ”€â”€ integration/  # Workflow testing
â”‚       â”œâ”€â”€ e2e/          # End-to-end testing
â”‚       â””â”€â”€ performance/  # Benchmarks
â”‚
â”œâ”€â”€ data/                  # Input data
â”‚   â”œâ”€â”€ datasets/         # 5 real-world log sources (497K entries)
â”‚   â”‚   â”œâ”€â”€ Apache/       # Web server logs (52K lines)
â”‚   â”‚   â”œâ”€â”€ HealthApp/    # Android health tracking (212K lines)
â”‚   â”‚   â”œâ”€â”€ Zookeeper/    # Distributed coordination (74K lines)
â”‚   â”‚   â”œâ”€â”€ OpenStack/    # Cloud infrastructure (137K lines)
â”‚   â”‚   â””â”€â”€ Proxifier/    # Network proxy (21K lines)
â”‚   â””â”€â”€ ground_truth/     # Manual annotations for validation
â”‚
â”œâ”€â”€ evaluation/           # Outputs & results
â”‚   â”œâ”€â”€ compressed/       # .lsc compressed files
â”‚   â”œâ”€â”€ results/          # Evaluation metrics (JSON/Markdown)
â”‚   â””â”€â”€ schema_versions/  # Schema evolution tracking
â”‚
â”œâ”€â”€ deployment/          # Infrastructure
â”‚   â”œâ”€â”€ Dockerfile       # Container image
â”‚   â”œâ”€â”€ docker-compose.yml# Service orchestration
â”‚   â””â”€â”€ Makefile         # Build automation
â”‚
â”œâ”€â”€ documentation/       # Project documentation
â”‚   â”œâ”€â”€ README.md        # Documentation index
â”‚   â”œâ”€â”€ TESTING.md       # Test strategy
â”‚   â”œâ”€â”€ MCP_ARCHITECTURE.md # System design
â”‚   â””â”€â”€ API.md           # Python API reference
â”‚
â””â”€â”€ scripts/            # Automation scripts
    â”œâ”€â”€ logsim-interactive.sh  # Bash interactive menu
    â”œâ”€â”€ run-tests.sh           # Test suite runner
    â””â”€â”€ run-pre-production-tests.sh # Validation
```

See individual README files in each directory for detailed information.

## ğŸ”¬ Research Methodology

### 1. Schema Extraction Pipeline

**6-Stage Process**:
1. **Tokenization**: FSM-based parser handles diverse log formats
2. **Semantic Classification**: Pattern-based field type detection (timestamp, IP, severity, etc.)
3. **Field Grouping**: Identify related fields (ip+port, user+action)
4. **Template Generation**: Log alignment algorithm extracts schemas
5. **Schema Versioning**: Track format evolution over time
6. **Validation**: Compare against manual ground truth (precision/recall)

**Example**:
```
Raw Logs:
  [Thu Jun 09 06:07:04 2005] [notice] LDAP: Built with OpenLDAP
  [Thu Jun 09 06:07:05 2005] [notice] LDAP: SSL support unavailable
  
Extracted Template:
  [TIMESTAMP] [SEVERITY] LDAP: [MESSAGE]
```

### 2. Semantic-Aware Compression

**Category-Specific Codecs**:
- **Timestamps**: Delta encoding (8-10Ã— compression)
- **Severity/Status**: Dictionary encoding (5-7Ã— compression)
- **Metrics**: Gorilla time-series compression (3-5Ã— compression)
- **Messages**: Token pool with references (variable)
- **Stack traces**: Reference tracking (store once, reuse pointer)

**Queryable Index**: Columnar storage enables filtering without full decompression.

### 3. Evaluation Metrics

**Accuracy** (vs manual annotations):
- Precision: % of extracted fields that are correct
- Recall: % of actual fields that were found
- F1-Score: Harmonic mean
- **Target**: >90% accuracy

**Compression Performance**:
- Compression ratio vs gzip baseline
- Query latency overhead
- **Target**: >10Ã— compression, <2Ã— query slowdown

## ğŸ§ª Testing

### Run Complete Test Suite

```bash
# All tests with coverage
bash scripts/run-tests.sh

# View coverage report
firefox htmlcov/index.html
```

### Pre-Production Validation

```bash
# Validate before deployment
bash scripts/run-pre-production-tests.sh
```

**Test Status**: âœ… 25/25 tests passing (100%)
- Unit tests: 9 tests
- Integration tests: 8 tests
- E2E tests: 3 tests
- Performance benchmarks: 5 tests

### Performance Benchmarks

```bash
# Run benchmarks
python -m pytest logsim/tests/performance/ --benchmark-only

# Expected results:
# - Compression: >500 ops/sec
# - Template extraction: >900 ops/sec
# - Linear scalability: 100 â†’ 10,000 logs
```

## ğŸ“š Documentation

- [Documentation Index](documentation/README.md) - Complete documentation overview
- [Testing Guide](documentation/TESTING.md) - Test strategy and commands
- [MCP Architecture](documentation/MCP_ARCHITECTURE.md) - System design details
- [API Reference](documentation/API.md) - Python API usage
- [Docker Guide](deployment/README.md) - Container deployment

## ğŸ“ Research Context

**Master's Thesis**: Automatic Schema Extraction from Unstructured System Logs  
**Duration**: 26 weeks (4 phases)  
**Target Venues**: VLDB, SIGMOD, IEEE BigData  
**Novel Contribution**: Semantic-aware compression adapting to log content types

### Related Work
- **Log Parsing**: Drain, Spell, LogPai
- **Schema Inference**: Lakehouse formats (Parquet, ORC)
- **Compression**: Generic (gzip, zstd) vs specialized (LogShrink)

### Key Differentiators
- âœ… No ML models (constraint-based approach)
- âœ… Semantic awareness (field-type-specific compression)
- âœ… Query preservation (columnar indexes)
- âœ… Schema evolution tracking
- âœ… Lossless compression (exact reconstruction)

## ğŸ› ï¸ Development

### Setup Development Environment

```bash
# Install test dependencies
pip install pytest pytest-cov pytest-benchmark pytest-mock

# Run tests on file changes (watch mode)
pip install pytest-watch
ptw logsim/tests/ -- -v
```

### Contribution Workflow

1. Create feature branch: `git checkout -b feature/new-encoder`
2. Make changes and add tests
3. Run validation: `bash scripts/run-pre-production-tests.sh`
4. Submit PR (GitHub Actions runs full test suite)

### Adding New Semantic Type Patterns

```python
# logsim/context/classification/semantic_types.py

def recognize_custom_field(token: str) -> Tuple[str, float]:
    """
    Add pattern for new field type.
    
    Returns:
        (field_type, confidence_score)
    """
    if re.match(r'^[A-Z]{3}-\d{4}$', token):
        return ('ERROR_CODE', 0.95)  # High confidence
    return ('UNKNOWN', 0.0)
```

### Adding New Compression Codecs

```python
# logsim/context/encoding/custom_encoder.py

from logsim.protocols import EncoderProtocol

class CustomEncoder(EncoderProtocol):
    def encode(self, values: List[Any]) -> bytes:
        # Your encoding logic
        pass
    
    def decode(self, data: bytes) -> List[Any]:
        # Your decoding logic
        pass
```

## ğŸ“¦ Dependencies

### Core Libraries
```
msgpack>=1.0.0          # Serialization
zstandard>=0.21.0       # Compression baseline
python-dateutil>=2.8.0  # Timestamp parsing
regex>=2023.0.0         # Advanced pattern matching
rich>=13.0.0            # Terminal UI
click>=8.1.0            # CLI framework
```

### Testing
```
pytest>=7.4.0           # Test framework
pytest-cov>=4.1.0       # Coverage reporting
pytest-benchmark>=4.0.0 # Performance testing
pytest-mock>=3.12.0     # Mocking utilities
```

### Optional Tools
```bash
# Baseline comparison
gzip --version

# Command-line benchmarking
cargo install hyperfine

# Memory profiling
pip install memory-profiler
```

## ğŸ³ Docker Deployment

### Build & Run

```bash
# Build all services
docker-compose -f deployment/docker-compose.yml build

# Run interactive CLI
docker-compose -f deployment/docker-compose.yml run --rm logsim-interactive

# Run compression
docker-compose -f deployment/docker-compose.yml run --rm logsim-cli \
  compress -i /app/data/datasets/Apache/Apache_full.log -o /app/evaluation/compressed/apache.lsc
```

### Environment Variables

```bash
# Set in docker-compose.yml
PYTHONUNBUFFERED=1      # Real-time output
TERM=xterm-256color     # Colored terminal
MIN_SUPPORT=3           # Template extraction threshold
ZSTD_LEVEL=15           # Compression level (1-22)
```

## ğŸ¤ Contributing

We welcome contributions! Please see CONTRIBUTING.md for guidelines.

### Areas for Contribution
- [ ] Additional semantic type patterns
- [ ] New compression codecs
- [ ] Query optimization
- [ ] Schema visualization
- [ ] Performance improvements

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.


## ğŸ”— Links

- [Project Documentation](documentation/README.md)
- [Test Results](evaluation/results/)
- [Research Roadmap](PROJECT.md)
- [GitHub Repository](https://github.com/adam-bouafia/LogSim)

## ğŸ“ Contact

- **Author**: Adam Bouafia
- **Repository**: https://github.com/adam-bouafia/LogSim
- **Linkedin**: https://www.linkedin.com/in/adam-bouafia 

---

**Status**: âœ… Production Ready | ğŸ§ª All Tests Passing (25/25) | ğŸ“Š Coverage: 42%

Built with â¤ï¸ for research in log analysis and semantic compression.
